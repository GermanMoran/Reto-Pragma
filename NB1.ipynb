{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias y Dependencias\n",
    "# =================================================\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import csv\n",
    "import datetime as dt\n",
    "import os\n",
    "import psycopg2\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple, Optional\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Concultas SQL de Inserción y Actualizacion \n",
    "# ====================================================\n",
    "\n",
    "INIT_STATS_ROW_SQL = \"\"\"\n",
    "INSERT INTO stats(name, cnt, ssum, smin, smax)\n",
    "VALUES('global', 0, 0.0, NULL, NULL)\n",
    "ON CONFLICT (name) DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "INSERT_TX_SQL = \"\"\"\n",
    "INSERT INTO transactions (timestamp, price, user_id, source_file)\n",
    "VALUES %s\n",
    "ON CONFLICT DO NOTHING\n",
    "\"\"\"\n",
    "\n",
    "UPDATE_STATS_SQL = \"\"\"\n",
    "UPDATE stats\n",
    "SET\n",
    "  cnt = cnt + %s,\n",
    "  ssum = ssum + %s,\n",
    "  smin = CASE WHEN smin IS NULL THEN %s ELSE LEAST(smin, %s) END,\n",
    "  smax = CASE WHEN smax IS NULL THEN %s ELSE GREATEST(smax, %s) END\n",
    "WHERE name = %s\n",
    "\"\"\"\n",
    "\n",
    "GET_STATS_SQL = \"SELECT cnt, ssum, smin, smax FROM stats WHERE name = %s\"\n",
    "\n",
    "\n",
    "\n",
    "CHECK_FILE_SQL = \"SELECT 1 FROM ingestion_log WHERE file_name = %s;\"\n",
    "\n",
    "\n",
    "LOG_FILE_SQL = \"\"\"\n",
    "INSERT INTO ingestion_log(file_name, rows_loaded, loaded_at)\n",
    "VALUES (%s, %s, %s)\n",
    "ON CONFLICT (file_name)\n",
    "DO UPDATE SET rows_loaded = EXCLUDED.rows_loaded,\n",
    "              loaded_at   = EXCLUDED.loaded_at;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Creacion de esquema de la Base de datos\n",
    "# ===============================================\n",
    "DDL = {\n",
    "    \"transactions\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS transactions (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            timestamp TIMESTAMP NOT NULL,\n",
    "            price DOUBLE PRECISION NOT NULL,\n",
    "            user_id TEXT NOT NULL,\n",
    "            source_file TEXT NOT NULL\n",
    "        );\n",
    "    \"\"\",\n",
    "    \"ingestion_log\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ingestion_log (\n",
    "            file_name   TEXT PRIMARY KEY,\n",
    "            rows_loaded INTEGER NOT NULL,\n",
    "            loaded_at   TIMESTAMP NOT NULL\n",
    "        );\n",
    "    \"\"\",\n",
    "    \"stats\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS stats (\n",
    "            name TEXT PRIMARY KEY CHECK (name = 'global'),\n",
    "            cnt  BIGINT NOT NULL,\n",
    "            ssum DOUBLE PRECISION,\n",
    "            smin DOUBLE PRECISION,\n",
    "            smax DOUBLE PRECISION \n",
    "        );\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Esta función me permite inicializar la BD, crear las tablas.\n",
    "\"\"\"\n",
    "def init_db(conn):\n",
    "    with conn.cursor() as cur:\n",
    "        for sql in DDL.values():\n",
    "            cur.execute(sql)\n",
    "        cur.execute(INIT_STATS_ROW_SQL)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias y Dependencias\n",
    "# =======================================================\n",
    "import os\n",
    "import csv\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from dateutil import parser as dateparser\n",
    "from decimal import Decimal, InvalidOperation\n",
    "\n",
    "\n",
    "\n",
    "# Variables Globales\n",
    "# ==================================================\n",
    "CSV_DIR = \"./datos\"\n",
    "MICROBATCH_SIZE = 8                  # poner 1 para actualizar por fila si quieres microbatch por fila\n",
    "SOURCE_PREFIX = \"2012-\"             # asumimos archivos 2012-1.csv ... 2012-5.csv\n",
    "STATS_NAME = 'global' \n",
    "\n",
    "\n",
    "\n",
    "# 1. Conexion BD Postgres\n",
    "# =====================================================\n",
    "DB_CONF = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"dbname\": \"mb\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"user\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def find_source_files(directory):\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "    # filter out validation.csv\n",
    "    files = [f for f in files if f.lower() != 'validation.csv']\n",
    "    # keep only those that start with SOURCE_PREFIX (opcional)\n",
    "    files = [f for f in files if f.startswith(SOURCE_PREFIX)]\n",
    "    # sort to ensure chronological order: \"2012-1.csv\", \"2012-2.csv\", ...\n",
    "    files.sort(key=lambda x: [int(part) if part.isdigit() else part for part in x.replace('.csv','').split('-')])\n",
    "    return files\n",
    "\n",
    "\n",
    "def parse_row(row):\n",
    "    # Asumimos campos: timestamp, price, user_id\n",
    "    ts_raw = row.get('timestamp') or row.get('time') or row.get('date')\n",
    "    price_raw = row.get('price')\n",
    "    user_id = row.get('user_id') or row.get('userid') or row.get('user')\n",
    "\n",
    "    if not price_raw or price_raw.strip() == '':\n",
    "        raise ValueError(\"price empty\")\n",
    "\n",
    "    # limpiar price (por si usan coma como separador)\n",
    "    p = price_raw.replace(',', '').strip()\n",
    "    try:\n",
    "        price = Decimal(p)\n",
    "    except InvalidOperation:\n",
    "        # intentar reemplazar posible formato dd/mm/yyyy por ejemplo (defensivo)\n",
    "        raise ValueError(f\"price invalid: {price_raw}\")\n",
    "\n",
    "    # parse timestamp robusto\n",
    "    ts = None\n",
    "    if ts_raw:\n",
    "        ts = dateparser.parse(ts_raw)\n",
    "    # else ts puede ser None\n",
    "\n",
    "    return ts, price, user_id\n",
    "\n",
    "def process_file(conn, filepath, source_file_name, microbatch_size=100):\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "        # Verificar si ya fue procesado\n",
    "    cur.execute(CHECK_FILE_SQL, (source_file_name,))\n",
    "    if cur.fetchone():\n",
    "        print(f\"[SKIP] Archivo {source_file_name} ya fue procesado previamente.\")\n",
    "        cur.close()\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    inserted_total = 0\n",
    "    batch = []\n",
    "    batch_prices = []\n",
    "\n",
    "    with open(filepath, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row_idx, row in enumerate(reader, start=1):\n",
    "            try:\n",
    "                ts, price, user_id = parse_row(row)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Skipping row {row_idx} in {source_file_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            batch.append((ts, price, user_id, source_file_name))\n",
    "            batch_prices.append(price)\n",
    "\n",
    "            if len(batch) >= microbatch_size:\n",
    "                # insert batch\n",
    "                execute_values(cur, INSERT_TX_SQL, batch, template=\"(%s, %s, %s, %s)\")\n",
    "                # compute batch aggregates\n",
    "                b_count = len(batch)\n",
    "                b_sum = sum(batch_prices)\n",
    "                b_min = min(batch_prices)\n",
    "                b_max = max(batch_prices)\n",
    "                # update stats atomically\n",
    "                cur.execute(UPDATE_STATS_SQL, (b_count, b_sum, b_min, b_min, b_max, b_max, STATS_NAME))\n",
    "                conn.commit()\n",
    "                inserted_total += b_count\n",
    "                print(f\"[INFO] Inserted batch of {b_count} rows from {source_file_name}. Total inserted so far for this file: {inserted_total}\")\n",
    "                batch.clear()\n",
    "                batch_prices.clear()\n",
    "\n",
    "    # leftover\n",
    "    if batch:\n",
    "        execute_values(cur, INSERT_TX_SQL, batch, template=\"(%s, %s, %s, %s)\")\n",
    "        b_count = len(batch)\n",
    "        b_sum = sum(batch_prices)\n",
    "        b_min = min(batch_prices)\n",
    "        b_max = max(batch_prices)\n",
    "        cur.execute(UPDATE_STATS_SQL, (b_count, b_sum, b_min, b_min, b_max, b_max, STATS_NAME))\n",
    "        conn.commit()\n",
    "        inserted_total += b_count\n",
    "        print(f\"[INFO] Inserted final batch of {b_count} rows from {source_file_name}. File total: {inserted_total}\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Registrar archivo en log\n",
    "    cur.execute(LOG_FILE_SQL, (source_file_name, inserted_total,datetime.now()))\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "\n",
    "    print(f\"[OK] Archivo {source_file_name} procesado con {inserted_total} registros insertados.\")\n",
    "\n",
    "    cur.close()\n",
    "    return inserted_total\n",
    "\n",
    "def print_stats(conn):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(GET_STATS_SQL, (STATS_NAME,))\n",
    "    row = cur.fetchone()\n",
    "    if row:\n",
    "        row_count, price_sum, price_min, price_max = row\n",
    "        mean = None\n",
    "        if row_count and row_count > 0:\n",
    "            mean = (price_sum / row_count) if price_sum is not None else None\n",
    "        print(f\"STATS -> rows: {row_count}, sum: {price_sum}, min: {price_min}, max: {price_max}, mean: {mean}\")\n",
    "    else:\n",
    "        print(\"No stats row found.\")\n",
    "    cur.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    conn = psycopg2.connect(**DB_CONF)\n",
    "    # Creacion de tablas\n",
    "    init_db(conn)\n",
    "    try:\n",
    "        #--- Procesar solo los archivos CSV ---\n",
    "        files = find_source_files(CSV_DIR)\n",
    "        if not files:\n",
    "            print(\"No se encontraron archivos de origen.\")\n",
    "            return\n",
    "\n",
    "        for fname in files:\n",
    "            fullpath = os.path.join(CSV_DIR, fname)\n",
    "            print(f\"\\n=== Procesando archivo {fname} ===\")\n",
    "            inserted = process_file(conn, fullpath, fname, microbatch_size=MICROBATCH_SIZE)\n",
    "            print(f\"[DONE] Archivo {fname} procesado. Filas insertadas: {inserted}\")\n",
    "            # Imprimir stats después de cada archivo\n",
    "            print_stats(conn)\n",
    "\n",
    "        # --- Paso 2: imprimir estadísticas acumuladas ---\n",
    "        print(\"\\n>>> Estadísticas acumuladas después de cargar todos los archivos 2012-*.csv\")\n",
    "        print_stats(conn)\n",
    "\n",
    "        # --- Paso 3: procesar validation.csv ---\n",
    "        val_path = os.path.join(CSV_DIR, \"validation.csv\")\n",
    "        if os.path.exists(val_path):\n",
    "            print(f\"\\n=== Procesando archivo validation.csv ===\")\n",
    "            inserted = process_file(conn, val_path, \"validation.csv\", microbatch_size=MICROBATCH_SIZE)\n",
    "            print(f\"[DONE] Archivo validation.csv procesado. Filas insertadas: {inserted}\")\n",
    "\n",
    "            # --- Paso 4: imprimir estadísticas después de validation ---\n",
    "            print(\"\\n>>> Estadísticas después de cargar validation.csv\")\n",
    "            print_stats(conn)\n",
    "        else:\n",
    "            print(\"\\n[WARN] No se encontró validation.csv en la carpeta de datos.\")\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Procesando archivo 2012-1.csv ===\n",
      "[INFO] Inserted batch of 8 rows from 2012-1.csv. Total inserted so far for this file: 8\n",
      "[WARN] Skipping row 12 in 2012-1.csv: price empty\n",
      "[INFO] Inserted batch of 8 rows from 2012-1.csv. Total inserted so far for this file: 16\n",
      "[WARN] Skipping row 21 in 2012-1.csv: price empty\n",
      "[INFO] Inserted final batch of 4 rows from 2012-1.csv. File total: 20\n",
      "[OK] Archivo 2012-1.csv procesado con 20 registros insertados.\n",
      "[DONE] Archivo 2012-1.csv procesado. Filas insertadas: 20\n",
      "STATS -> rows: 20, sum: 1193.0, min: 14.0, max: 97.0, mean: 59.65\n",
      "\n",
      "=== Procesando archivo 2012-2.csv ===\n",
      "[INFO] Inserted batch of 8 rows from 2012-2.csv. Total inserted so far for this file: 8\n",
      "[INFO] Inserted batch of 8 rows from 2012-2.csv. Total inserted so far for this file: 16\n",
      "[INFO] Inserted batch of 8 rows from 2012-2.csv. Total inserted so far for this file: 24\n",
      "[INFO] Inserted final batch of 5 rows from 2012-2.csv. File total: 29\n",
      "[OK] Archivo 2012-2.csv procesado con 29 registros insertados.\n",
      "[DONE] Archivo 2012-2.csv procesado. Filas insertadas: 29\n",
      "STATS -> rows: 49, sum: 2783.0, min: 10.0, max: 100.0, mean: 56.795918367346935\n",
      "\n",
      "=== Procesando archivo 2012-3.csv ===\n",
      "[INFO] Inserted batch of 8 rows from 2012-3.csv. Total inserted so far for this file: 8\n",
      "[INFO] Inserted batch of 8 rows from 2012-3.csv. Total inserted so far for this file: 16\n",
      "[INFO] Inserted batch of 8 rows from 2012-3.csv. Total inserted so far for this file: 24\n",
      "[INFO] Inserted final batch of 7 rows from 2012-3.csv. File total: 31\n",
      "[OK] Archivo 2012-3.csv procesado con 31 registros insertados.\n",
      "[DONE] Archivo 2012-3.csv procesado. Filas insertadas: 31\n",
      "STATS -> rows: 80, sum: 4633.0, min: 10.0, max: 100.0, mean: 57.9125\n",
      "\n",
      "=== Procesando archivo 2012-4.csv ===\n",
      "[INFO] Inserted batch of 8 rows from 2012-4.csv. Total inserted so far for this file: 8\n",
      "[INFO] Inserted batch of 8 rows from 2012-4.csv. Total inserted so far for this file: 16\n",
      "[WARN] Skipping row 17 in 2012-4.csv: price empty\n",
      "[INFO] Inserted batch of 8 rows from 2012-4.csv. Total inserted so far for this file: 24\n",
      "[WARN] Skipping row 27 in 2012-4.csv: price empty\n",
      "[INFO] Inserted final batch of 4 rows from 2012-4.csv. File total: 28\n",
      "[OK] Archivo 2012-4.csv procesado con 28 registros insertados.\n",
      "[DONE] Archivo 2012-4.csv procesado. Filas insertadas: 28\n",
      "STATS -> rows: 108, sum: 6240.0, min: 10.0, max: 100.0, mean: 57.77777777777778\n",
      "\n",
      "=== Procesando archivo 2012-5.csv ===\n",
      "[INFO] Inserted batch of 8 rows from 2012-5.csv. Total inserted so far for this file: 8\n",
      "[INFO] Inserted batch of 8 rows from 2012-5.csv. Total inserted so far for this file: 16\n",
      "[INFO] Inserted batch of 8 rows from 2012-5.csv. Total inserted so far for this file: 24\n",
      "[INFO] Inserted final batch of 7 rows from 2012-5.csv. File total: 31\n",
      "[OK] Archivo 2012-5.csv procesado con 31 registros insertados.\n",
      "[DONE] Archivo 2012-5.csv procesado. Filas insertadas: 31\n",
      "STATS -> rows: 139, sum: 8046.0, min: 10.0, max: 100.0, mean: 57.884892086330936\n",
      "\n",
      ">>> Estadísticas acumuladas después de cargar todos los archivos 2012-*.csv\n",
      "STATS -> rows: 139, sum: 8046.0, min: 10.0, max: 100.0, mean: 57.884892086330936\n",
      "\n",
      "=== Procesando archivo validation.csv ===\n",
      "[INFO] Inserted batch of 8 rows from validation.csv. Total inserted so far for this file: 8\n",
      "[OK] Archivo validation.csv procesado con 8 registros insertados.\n",
      "[DONE] Archivo validation.csv procesado. Filas insertadas: 8\n",
      "\n",
      ">>> Estadísticas después de cargar validation.csv\n",
      "STATS -> rows: 147, sum: 8380.0, min: 10.0, max: 100.0, mean: 57.006802721088434\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpragma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
